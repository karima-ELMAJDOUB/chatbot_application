# -*- coding: utf-8 -*-
"""preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sNblUjQa7gmP6Ow4pP16GDhfMsjJPc4Z
"""

import nltk
from nltk.stem import WordNetLemmatizer
import json
import pickle
import spacy
import numpy as np

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('omw')

def clean_up_sentence(sentence):
    lemmatizer = WordNetLemmatizer()
    sentence_words = nltk.word_tokenize(sentence)
    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]
    return sentence_words

def bow(sentence, words, show_details=True):
    sentence_words = clean_up_sentence(sentence)
    bag = [0] * len(words)
    for s in sentence_words:
        for i, w in enumerate(words):
            if w == s:
                bag[i] = 1
                if show_details:
                    print("found in bag: %s" % w)
    return np.array(bag)

def preprocess_data(data_file_path):
    with open(data_file_path, 'r') as file:
        data = json.load(file)

    words = []
    classes = []
    documents = []

    for intent in data['intents']:
        for pattern in intent['patterns']:
            w = nltk.word_tokenize(pattern)
            words.extend(w)
            documents.append((w, intent['tag']))
            if intent['tag'] not in classes:
                classes.append(intent['tag'])

    nlp = spacy.load("en_core_web_sm")
    documents_lemmatized = []

    for doc_tuple in documents:
        doc_text = doc_tuple[0]
        if isinstance(doc_text, list):
            doc_text = " ".join(doc_text)
        doc_spacy = nlp(doc_text)
        lemmatized_tokens = [token.lemma_ for token in doc_spacy if not token.is_stop and not token.is_punct]
        lemmatized_doc = " ".join(lemmatized_tokens)
        documents_lemmatized.append(lemmatized_doc)

    words = set([word for doc in documents_lemmatized for word in doc.split()])
    words = sorted(list(words))
    classes = sorted(list(set(classes)))

    with open('words.pkl', 'wb') as file:
        pickle.dump(words, file)

    with open('classes.pkl', 'wb') as file:
        pickle.dump(classes, file)

    return words, classes, documents_lemmatized