# -*- coding: utf-8 -*-
"""preprocess_data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qUHhUDcXdy-PJZDR9TVeNgDGpIZPFjqP
"""

# preprocess_data.py

import nltk
from nltk.stem import WordNetLemmatizer
import json
import pickle
import spacy
import numpy as np

nltk.download('punkt')

nlp = spacy.load("en_core_web_sm")
lemmatizer = WordNetLemmatizer()

def clean_up_sentence(sentence):
    sentence_words = nltk.word_tokenize(sentence)
    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]
    return sentence_words

def bow(sentence, words):
    sentence_words = clean_up_sentence(sentence)
    bag = [1 if w in sentence_words else 0 for w in words]
    return np.array(bag)

def preprocess_data(intents_file):
    words = []
    classes = []
    documents = []
    lemmatized_documents = []  # Ajout de cette liste pour stocker les documents lemmatizés

    ignore_words = ['?', '!']

    with open(intents_file) as file:
        intents = json.load(file)

    for intent in intents['intents']:
        for pattern in intent['patterns']:
            w = nltk.word_tokenize(pattern)
            words.extend(w)
            documents.append((w, intent['tag']))
            if intent['tag'] not in classes:
                classes.append(intent['tag'])

    for doc_tuple in documents:
        doc_text = doc_tuple[0]
        if isinstance(doc_text, list):
            doc_text = " ".join(doc_text)
        doc_spacy = nlp(doc_text)
        lemmatized_tokens = [token.lemma_ for token in doc_spacy if not token.is_stop and not token.is_punct]
        lemmatized_doc = " ".join(lemmatized_tokens)
        lemmatized_documents.append(lemmatized_doc)

    words = set([word for doc in lemmatized_documents for word in doc.split() if word not in ignore_words])
    words = sorted(list(words))
    classes = sorted(list(set(classes)))

    pickle.dump(words, open('words.pkl', 'wb'))
    pickle.dump(classes, open('classes.pkl', 'wb'))

    # Sauvegarder les données prétraitées pour une utilisation ultérieure
    with open('lemmatized_documents.pkl', 'wb') as file:
        pickle.dump(lemmatized_documents, file)

    return lemmatized_documents, words, classes

if __name__ == "__main__":
    intents_file_path = '/content/intents.json'  # Update with the correct path
    preprocess_data(intents_file_path)